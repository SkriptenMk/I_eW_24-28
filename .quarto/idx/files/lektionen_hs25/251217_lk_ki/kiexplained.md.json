{"title":"20 KI Explained","markdown":{"yaml":{"title":"20 KI Explained","date":"2025-12-17","date-format":"DD.MM.YYYY","author":"Peter Rutschmann"},"headingText":"KI Explained","containsRefs":false,"markdown":"\n\n\n_\"Viele KIs k√∂nnen reden.<br/>\nGute KIs wurden erzogen.\"_\n\n(und Erziehung ist hier bewusst gew√§hlt üòâ)\n\n---\n\n> ## Aufgabe: Welche KI kennen Sie?\n>\n> - Von welchen KI haben Sie schon geh√∂rt?\n> - Bei welchen T√§tigkeiten lassen Sie sich von KI unterst√ºtzen?\n> - Welche KI benutzen Sie regelm√§ssig?\n\n---\n\n## Wie funktioniert KI?\n\nEine KI versteht keine Inhalte --> Sie erkennt Muster in Daten.\n\nK√ºnstliche Intelligenz (KI) ist ein Sammelbegriff f√ºr Systeme, die:\n\n- aus Daten lernen\n- Muster erkennen\n- Vorhersagen treffen\n- Entscheidungen simulieren\n\n**Aber:** Nicht alles, was KI heisst, kann sprechen, mit Ihnen chatten oder Texte schreiben.\n\n## Arten von KI:\n\n- Regelbasierte Systeme: Folgen festen Regeln (z. B. Chatbots mit Skripten)\n- Maschinelles Lernen: Lernen aus Daten (z. B. Bilderkennung)\n- Tiefe neuronale Netze: Komplexe Modelle, die grosse Datenmengen verarbeiten (z. B. Sprachmodelle, wie GPT-5 von OpenAI)\n\n### Sprachmodelle -KI\n\nBeispiele:\n\n- ChatGPT\n- andere Chatbots\n- Text-Generatoren\n\nWas sie k√∂nnen:\n\n- Texte schreiben\n- erkl√§ren\n- zusammenfassen\n- Fragen beantworten\n\nWie sie es tun:\n\n- Muster in riesigen Textmengen erkennen\n- Wahrscheinlichkeiten berechnen\n- Texte generieren\n\n### Bild erzeugende -KI\n\nBeispiele:\n\n- DALL¬∑E\n- Midjourney\n- Stable Diffusion\n\nWas sie k√∂nnen:\n\n- Bilder generieren\n- Fotos analysieren\n- Gesichter erkennen\n- Objekte z√§hlen\n\nBeispiel aus dem Alltag\n\n- Gesichtserkennung im Smartphone\n- automatische Bildverbesserung\n\n### Audio- & Sprach-KI\n\nBeispiele:\n\n- Text-to-Speech (Vorlesen)\n- Speech-to-Text (Diktieren)\n- Sprachassistenten (z. B. Siri, Alexa)\n\nWas sie k√∂nnen\n\n- gesprochene Sprache erkennen\n- Stimmen erzeugen\n\n### Empfehlungs-KI (sehr verbreitet!)\n\nBeispiele\n\n- Netflix\n- YouTube\n- Spotify\n- Online-Shops\n\nWas sie k√∂nnen:\n\n- Inhalte vorschlagen\n- personalisierte Werbung zeigen\n- Vorhersagen, was dir gefallen k√∂nnte\n\nViele User benutzen diese KI t√§glich, ohne sie als KI zu erkennen.\n\n### Entscheidungs- & Prognose-KI\n\nBeispiele:\n\n- Kredit-Bewertung\n- Betrugserkennung\n- Verkehrsprognosen\n- Wettermodelle\n\nWas sie k√∂nnen:\n\n- Risiken absch√§tzen\n\n### Steuerungs- & Regelungs-KI\n\nBeispiele\n\n- Fahrassistenzsysteme\n- Robotik\n- industrielle Anlagen\n\nWas sie k√∂nnen:\n\n- Maschinen steuern\n- auf Sensordaten reagieren\n- Prozesse optimieren\n\n---\n\n## Wie funktioniert ein Sprachmodell?\n\nEs gibt also ganz viele verschiedene Arten von KIs.<br/>\nFokusieren wir uns zun√§chst auf auf Sprachmodelle wie GPT-5.\n\nEine solche KI wird mit riesigen Textmengen trainiert.<br/>\nSie liest sehr viele Texte und erkennt, welche W√∂rter h√§ufig aufeinander folgen<br/>\nDaraus berechnet sie Wahrscheinlichkeiten, welches Wort als n√§chstes kommt.\n\nBeispiel:\n\n- \"Die Katze sitzt auf dem ___\"\n- M√∂gliche W√∂rter: \"Dach\", \"Tisch\", \"Boden\", \"Auto\"\n- Wahrscheinlichkeiten: \"Dach\" 40%, \"Tisch\" 30%, \"Boden\" 20%, \"Auto\" 10%\n\nDie KI w√§hlt dann das Wort mit der h√∂chsten Wahrscheinlichkeit aus und f√ºgt es dem Text hinzu.<br/>\nDann wiederholt sie den Prozess f√ºr das n√§chste Wort, bis der Text fertig ist.\n\n==> KI denkt nicht ‚Äì sie vervollst√§ndigt.\n\n### KI mit Zugriff auf das Internet\n\nModerne KIs k√∂nnen auch auf das Internet zugreifen.<br/>\nSie k√∂nnen Informationen in Echtzeit abrufen und in ihre Antworten einbauen.\n\nBeispiel: \"Wie ist das Wetter heute in Z√ºrich?\"<br/>\nDie KI kann die aktuelle Wettervorhersage abrufen und dem Benutzer mitteilen.\n\nHier arbeitet die KI also nicht nur mit den Mustern aus ihren Trainingsdaten, sondern auch mit aktuellen Daten aus dem Internet.\n\n### Mehr als nur Worter aneinanderreihen\n\nModerne Sprachmodelle nutzen auch Kontextinformationen.<br/>\nSie ber√ºcksichtigen den gesamten Satz oder sogar den ganzen Text, um bessere Vorhersagen zu treffen.<br/>\nSie k√∂nnen auch grammatikalische Regeln und Stilrichtungen lernen, um nat√ºrlichere Texte zu erzeugen.\n\n### Auf den Benutzer eingehen\n\nEin grosse St√§rke der heutigen Sprachmodelle, ist dass sie auf den Benutzer eingehen k√∂nnen.<br/>\nSie geben dem Benutzer das Gef√ºhl, verstanden zu werden.<br/>\nSie loben den Benutzer, wenn er gute Fragen stellt.<br/>\nSie sind h√∂flich und respektvoll. (Ausser man sagt ihnen, dass sie unh√∂flich sein sollen.)<br/>\nSprachmodelle k√∂nnen auch auf den Benutzer eingehen.<br/>\n\nBeispiel:\n\n- Benutzer: \"Erkl√§re mir Quantenphysik einfach.\"\n- KI: \"Klar! Quantenphysik ist die Wissenschaft, die sich mit den kleinsten Teilchen im Universum besch√§ftigt...\"\n\n**Wichtig:** Die KI versteht den Inhalt nicht wirklich. Sie erkennt nur Muster und Wahrscheinlichkeiten in den Texten, die sie gelesen hat.<br/>\n**Sie wurde allerdings auch so trainiert, dass sie auf den Benutzer eingeht** und passende freundliche Antworten generiert. Und auch das sind f√ºr die KI nur Muster auf der Basis der Trainingsdaten.\n\nDie Kunst der heutigen KI ist es dann auch, dass wir empfinden:\n\n- KI hat ein grosses Wissen\n- KI geht auf mich ein\n\nDoch unterscheidet die KI nicht zwischen Wissen und Benutzerinteraktion.<br/>\nSie hat nur ein Modell, das beides kombiniert.\n\nWir wollen uns diese Aspekte etwas genauer anschauen.<br/>\nEine komplexe KI zu verstehen oder zu trainieren √ºbersteigt unsere M√∂glichkeiten.<br/>\nAber es gibt einfachere Modelle, die uns dennoch einen einfachen Einblick geben k√∂nnen.\n\n---\n\n> ## Aufgabe: Rohes Sprachmodell, Teil 1\n>\n> Entdecken Sie mit einfachen Sprachmodellen die Wirkung von Training zum Resultat.\n>\n> Das k√∂nnen Sie dabei erleben/ sp√ºhren / erforschen:\n>\n>    - Trainieren eines Modells mit eigenen Texten\n>    - Generieren von neuen Texten basierend auf dem trainierten Modell\n>    - Allderings ist das Ergebnis sehr einfach und nicht so ausgefeilt wie bei modernen KIs.\n>\n> [Soekia](https://www.soekia.ch/GPT/) zeigt das Vorgehen sehr inuitiv.<br/>\n> Und Sie vorgegebene Trainingsdaten ausw√§hlen. Und dann das Modell ausprobieren.<br/>\n> Probieren Sie es aus mit verschiedenen Trainingsdaten.\n> Oder Sie geben selber Trainingsdaten vor. Und probieren die Wirkung aus.\n>\n> Eine gute spielersiche M√∂glichkeit ist der Einstieg mit TicTacToe.<br/>\n>\n>  <img src=\"./chapter/soekia.png\" alt=\"Soekia TicTacToe Website\" width=\"700\"/>\n>\n> Was aber nicht erlebbar ist:\n>\n>    - Kein Dialog\n>    - Kein Kontextverst√§ndnis\n>.   - So sieht KI ohne Feinlenkung / Erziehung aus.\n\n---\n\n> ## Aufgabe: Rohes Sprachmodell, Teil 2\n>\n> Der Markov-Text-Generator ist ein andere M√∂glichkeit f√ºr ein \"rohes Sprachmodell\".<br/>\n> Von der Art und Weise vergleichbar mit dem Soekia Modell.<br/>\n>\n> Probieren Sie es aus: [markovtextgenerator](https://lehrertools.eu/ki)\n>\n> Geben Sie eine Text ein.<br/>\nZum Beispiel das Lied: _Auf der Mauer, auf der Lauer sitzt 'ne kleine Wanze. Auf der Mauer, auf der Lauer sitzt 'ne kleine Wanze. Seht euch mal die Wanze an, wie die Wanze tanzen kann, auf der Mauer, auf der Lauer sitzt 'ne kleine Wanze._\n> \n> Dann geben Sie nur ein Wort ein. zBsp Wanze:\n> \n>Der Generator erzeugt einen Text basierend auf den Wahrscheinlichkeiten der Wortfolgen im Trainingsmaterial.\n>\n>   - Versuchen Sie es mit einem weiteren Text und verschiedenen Startw√∂rtern.\n>   - Versuchen Sie zu erkennen, wie die Wortfolgen aus dem Trainingsmaterial √ºbernommen werden.\n>   - Varieren Sie die Trainingsdaten und beobachen Sie ob Sie das Ergebnis beeinflussen k√∂nnen.\n> \n\n---\n\n> ## Aufgabe: Starkes Modell ohne Dialoglenkung\n>\n> Ein Raw-LLM (Text Completion) ist zwar ein starkes Sprachmodell, hat aber keine Dialoglenkung\n>\n> Die Modelle sind bereits trainiert. Sie m√ºssen also keine Trainingsdaten eingeben.<br/>\n> Wenn Sie ein Text-Fragment eingeben, dann k√∂nnen die Modelle die wahrscheinlichsten n√§chsten W√∂rter vorhersagen.\n>\n> **M√∂glichkeite um es auszuprobieren:**<br/>\n> \n> Hier kann man gleich loslegen, Text eingeben und das Modell das n√§chste Wort herausfinden lassen. [banana project](https://banana-projects-transformer-autocomplete.hf.space/doc/gpt2-large)<br/>\n>\n> Diese folgenden Links brauchen etwas mehr Aufwand. Da muss man sich richtig Zeit nehmen.(sprengt unseren Rahmen im Unterricht)<br/>\n>\n> - Ein Playground von openai selber. Leider nur mit Kreditkarte: [OpenAI Playground](https://platform.openai.com/playground)<br/>\n>\n> - Ein [Jupyter Notebook auf Colab](https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb?utm_source=chatgpt.com) mit GPT-2. Allerdings nicht ganz ohne. Genau lesen und den Anweisungen folgen.\n\n---\n\n> ## Aufgabe Instruktions- & Dialog-KI\n> \n> heutige Chat-KIs (bewusst genutzt): Mini-Experiment (5 Minuten)<br/>\n>\n> Gleiche Aufgabe auf zwei Arten l√∂sen:<br/>\n>\n> Variante A: Eine sehr ‚Äûrohen‚Äú Prompt eingeben<br/>\n> ‚ÄûErkl√§re den Unterschied zwischen Frontend und Backend.‚Äú<br/>\n\n> Variante B ‚Äì dialogisch<br/>\n> \"Erkl√§re mir den Unterschied zwischen Frontend und Backend f√ºr einen Lernenden im 1. Lehrjahr. Stelle mir zuerst eine R√ºckfrage.\"\n>\n> Anschliessend Reflexion<br/>\n> Woher kommt:\n>\n>   - H√∂flichkeit?\n>   - Struktur?\n>   - Kontextverst√§ndnis?\n>   - Benutzerorientierung?\n>\n\n---\n\n> ## Aufgabe Ein guter Prompt\n> \n> Fragen Sie eine aktuelle Chat-KI, wie man einen guten Prompt schreibt.<br/>\n> Stellen Sie weitere Fragen, gehen Sie mit der Ki tiefer in das Thema hinein.<br/>\n> Wenden Sie dabei direkt an, was Sie gerade von der KI erkl√§rt bekommen haben.\n>\n> Tauschen Sie sich mit Ihrem Lernpartner aus.<br/>\n> Wie ein guter Prompt aufgebaut sein muss.\n> \n> Reflextion:\n>\n>   - Wieso wirkt sich ein guter Prompt so stark auf die Antwort der KI aus?\n>   - Wie kann ich das Wissen √ºber gute Prompts in Zukunft nutzen?\n>\n\n---","srcMarkdownNoYaml":"\n\n# KI Explained\n\n_\"Viele KIs k√∂nnen reden.<br/>\nGute KIs wurden erzogen.\"_\n\n(und Erziehung ist hier bewusst gew√§hlt üòâ)\n\n---\n\n> ## Aufgabe: Welche KI kennen Sie?\n>\n> - Von welchen KI haben Sie schon geh√∂rt?\n> - Bei welchen T√§tigkeiten lassen Sie sich von KI unterst√ºtzen?\n> - Welche KI benutzen Sie regelm√§ssig?\n\n---\n\n## Wie funktioniert KI?\n\nEine KI versteht keine Inhalte --> Sie erkennt Muster in Daten.\n\nK√ºnstliche Intelligenz (KI) ist ein Sammelbegriff f√ºr Systeme, die:\n\n- aus Daten lernen\n- Muster erkennen\n- Vorhersagen treffen\n- Entscheidungen simulieren\n\n**Aber:** Nicht alles, was KI heisst, kann sprechen, mit Ihnen chatten oder Texte schreiben.\n\n## Arten von KI:\n\n- Regelbasierte Systeme: Folgen festen Regeln (z. B. Chatbots mit Skripten)\n- Maschinelles Lernen: Lernen aus Daten (z. B. Bilderkennung)\n- Tiefe neuronale Netze: Komplexe Modelle, die grosse Datenmengen verarbeiten (z. B. Sprachmodelle, wie GPT-5 von OpenAI)\n\n### Sprachmodelle -KI\n\nBeispiele:\n\n- ChatGPT\n- andere Chatbots\n- Text-Generatoren\n\nWas sie k√∂nnen:\n\n- Texte schreiben\n- erkl√§ren\n- zusammenfassen\n- Fragen beantworten\n\nWie sie es tun:\n\n- Muster in riesigen Textmengen erkennen\n- Wahrscheinlichkeiten berechnen\n- Texte generieren\n\n### Bild erzeugende -KI\n\nBeispiele:\n\n- DALL¬∑E\n- Midjourney\n- Stable Diffusion\n\nWas sie k√∂nnen:\n\n- Bilder generieren\n- Fotos analysieren\n- Gesichter erkennen\n- Objekte z√§hlen\n\nBeispiel aus dem Alltag\n\n- Gesichtserkennung im Smartphone\n- automatische Bildverbesserung\n\n### Audio- & Sprach-KI\n\nBeispiele:\n\n- Text-to-Speech (Vorlesen)\n- Speech-to-Text (Diktieren)\n- Sprachassistenten (z. B. Siri, Alexa)\n\nWas sie k√∂nnen\n\n- gesprochene Sprache erkennen\n- Stimmen erzeugen\n\n### Empfehlungs-KI (sehr verbreitet!)\n\nBeispiele\n\n- Netflix\n- YouTube\n- Spotify\n- Online-Shops\n\nWas sie k√∂nnen:\n\n- Inhalte vorschlagen\n- personalisierte Werbung zeigen\n- Vorhersagen, was dir gefallen k√∂nnte\n\nViele User benutzen diese KI t√§glich, ohne sie als KI zu erkennen.\n\n### Entscheidungs- & Prognose-KI\n\nBeispiele:\n\n- Kredit-Bewertung\n- Betrugserkennung\n- Verkehrsprognosen\n- Wettermodelle\n\nWas sie k√∂nnen:\n\n- Risiken absch√§tzen\n\n### Steuerungs- & Regelungs-KI\n\nBeispiele\n\n- Fahrassistenzsysteme\n- Robotik\n- industrielle Anlagen\n\nWas sie k√∂nnen:\n\n- Maschinen steuern\n- auf Sensordaten reagieren\n- Prozesse optimieren\n\n---\n\n## Wie funktioniert ein Sprachmodell?\n\nEs gibt also ganz viele verschiedene Arten von KIs.<br/>\nFokusieren wir uns zun√§chst auf auf Sprachmodelle wie GPT-5.\n\nEine solche KI wird mit riesigen Textmengen trainiert.<br/>\nSie liest sehr viele Texte und erkennt, welche W√∂rter h√§ufig aufeinander folgen<br/>\nDaraus berechnet sie Wahrscheinlichkeiten, welches Wort als n√§chstes kommt.\n\nBeispiel:\n\n- \"Die Katze sitzt auf dem ___\"\n- M√∂gliche W√∂rter: \"Dach\", \"Tisch\", \"Boden\", \"Auto\"\n- Wahrscheinlichkeiten: \"Dach\" 40%, \"Tisch\" 30%, \"Boden\" 20%, \"Auto\" 10%\n\nDie KI w√§hlt dann das Wort mit der h√∂chsten Wahrscheinlichkeit aus und f√ºgt es dem Text hinzu.<br/>\nDann wiederholt sie den Prozess f√ºr das n√§chste Wort, bis der Text fertig ist.\n\n==> KI denkt nicht ‚Äì sie vervollst√§ndigt.\n\n### KI mit Zugriff auf das Internet\n\nModerne KIs k√∂nnen auch auf das Internet zugreifen.<br/>\nSie k√∂nnen Informationen in Echtzeit abrufen und in ihre Antworten einbauen.\n\nBeispiel: \"Wie ist das Wetter heute in Z√ºrich?\"<br/>\nDie KI kann die aktuelle Wettervorhersage abrufen und dem Benutzer mitteilen.\n\nHier arbeitet die KI also nicht nur mit den Mustern aus ihren Trainingsdaten, sondern auch mit aktuellen Daten aus dem Internet.\n\n### Mehr als nur Worter aneinanderreihen\n\nModerne Sprachmodelle nutzen auch Kontextinformationen.<br/>\nSie ber√ºcksichtigen den gesamten Satz oder sogar den ganzen Text, um bessere Vorhersagen zu treffen.<br/>\nSie k√∂nnen auch grammatikalische Regeln und Stilrichtungen lernen, um nat√ºrlichere Texte zu erzeugen.\n\n### Auf den Benutzer eingehen\n\nEin grosse St√§rke der heutigen Sprachmodelle, ist dass sie auf den Benutzer eingehen k√∂nnen.<br/>\nSie geben dem Benutzer das Gef√ºhl, verstanden zu werden.<br/>\nSie loben den Benutzer, wenn er gute Fragen stellt.<br/>\nSie sind h√∂flich und respektvoll. (Ausser man sagt ihnen, dass sie unh√∂flich sein sollen.)<br/>\nSprachmodelle k√∂nnen auch auf den Benutzer eingehen.<br/>\n\nBeispiel:\n\n- Benutzer: \"Erkl√§re mir Quantenphysik einfach.\"\n- KI: \"Klar! Quantenphysik ist die Wissenschaft, die sich mit den kleinsten Teilchen im Universum besch√§ftigt...\"\n\n**Wichtig:** Die KI versteht den Inhalt nicht wirklich. Sie erkennt nur Muster und Wahrscheinlichkeiten in den Texten, die sie gelesen hat.<br/>\n**Sie wurde allerdings auch so trainiert, dass sie auf den Benutzer eingeht** und passende freundliche Antworten generiert. Und auch das sind f√ºr die KI nur Muster auf der Basis der Trainingsdaten.\n\nDie Kunst der heutigen KI ist es dann auch, dass wir empfinden:\n\n- KI hat ein grosses Wissen\n- KI geht auf mich ein\n\nDoch unterscheidet die KI nicht zwischen Wissen und Benutzerinteraktion.<br/>\nSie hat nur ein Modell, das beides kombiniert.\n\nWir wollen uns diese Aspekte etwas genauer anschauen.<br/>\nEine komplexe KI zu verstehen oder zu trainieren √ºbersteigt unsere M√∂glichkeiten.<br/>\nAber es gibt einfachere Modelle, die uns dennoch einen einfachen Einblick geben k√∂nnen.\n\n---\n\n> ## Aufgabe: Rohes Sprachmodell, Teil 1\n>\n> Entdecken Sie mit einfachen Sprachmodellen die Wirkung von Training zum Resultat.\n>\n> Das k√∂nnen Sie dabei erleben/ sp√ºhren / erforschen:\n>\n>    - Trainieren eines Modells mit eigenen Texten\n>    - Generieren von neuen Texten basierend auf dem trainierten Modell\n>    - Allderings ist das Ergebnis sehr einfach und nicht so ausgefeilt wie bei modernen KIs.\n>\n> [Soekia](https://www.soekia.ch/GPT/) zeigt das Vorgehen sehr inuitiv.<br/>\n> Und Sie vorgegebene Trainingsdaten ausw√§hlen. Und dann das Modell ausprobieren.<br/>\n> Probieren Sie es aus mit verschiedenen Trainingsdaten.\n> Oder Sie geben selber Trainingsdaten vor. Und probieren die Wirkung aus.\n>\n> Eine gute spielersiche M√∂glichkeit ist der Einstieg mit TicTacToe.<br/>\n>\n>  <img src=\"./chapter/soekia.png\" alt=\"Soekia TicTacToe Website\" width=\"700\"/>\n>\n> Was aber nicht erlebbar ist:\n>\n>    - Kein Dialog\n>    - Kein Kontextverst√§ndnis\n>.   - So sieht KI ohne Feinlenkung / Erziehung aus.\n\n---\n\n> ## Aufgabe: Rohes Sprachmodell, Teil 2\n>\n> Der Markov-Text-Generator ist ein andere M√∂glichkeit f√ºr ein \"rohes Sprachmodell\".<br/>\n> Von der Art und Weise vergleichbar mit dem Soekia Modell.<br/>\n>\n> Probieren Sie es aus: [markovtextgenerator](https://lehrertools.eu/ki)\n>\n> Geben Sie eine Text ein.<br/>\nZum Beispiel das Lied: _Auf der Mauer, auf der Lauer sitzt 'ne kleine Wanze. Auf der Mauer, auf der Lauer sitzt 'ne kleine Wanze. Seht euch mal die Wanze an, wie die Wanze tanzen kann, auf der Mauer, auf der Lauer sitzt 'ne kleine Wanze._\n> \n> Dann geben Sie nur ein Wort ein. zBsp Wanze:\n> \n>Der Generator erzeugt einen Text basierend auf den Wahrscheinlichkeiten der Wortfolgen im Trainingsmaterial.\n>\n>   - Versuchen Sie es mit einem weiteren Text und verschiedenen Startw√∂rtern.\n>   - Versuchen Sie zu erkennen, wie die Wortfolgen aus dem Trainingsmaterial √ºbernommen werden.\n>   - Varieren Sie die Trainingsdaten und beobachen Sie ob Sie das Ergebnis beeinflussen k√∂nnen.\n> \n\n---\n\n> ## Aufgabe: Starkes Modell ohne Dialoglenkung\n>\n> Ein Raw-LLM (Text Completion) ist zwar ein starkes Sprachmodell, hat aber keine Dialoglenkung\n>\n> Die Modelle sind bereits trainiert. Sie m√ºssen also keine Trainingsdaten eingeben.<br/>\n> Wenn Sie ein Text-Fragment eingeben, dann k√∂nnen die Modelle die wahrscheinlichsten n√§chsten W√∂rter vorhersagen.\n>\n> **M√∂glichkeite um es auszuprobieren:**<br/>\n> \n> Hier kann man gleich loslegen, Text eingeben und das Modell das n√§chste Wort herausfinden lassen. [banana project](https://banana-projects-transformer-autocomplete.hf.space/doc/gpt2-large)<br/>\n>\n> Diese folgenden Links brauchen etwas mehr Aufwand. Da muss man sich richtig Zeit nehmen.(sprengt unseren Rahmen im Unterricht)<br/>\n>\n> - Ein Playground von openai selber. Leider nur mit Kreditkarte: [OpenAI Playground](https://platform.openai.com/playground)<br/>\n>\n> - Ein [Jupyter Notebook auf Colab](https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb?utm_source=chatgpt.com) mit GPT-2. Allerdings nicht ganz ohne. Genau lesen und den Anweisungen folgen.\n\n---\n\n> ## Aufgabe Instruktions- & Dialog-KI\n> \n> heutige Chat-KIs (bewusst genutzt): Mini-Experiment (5 Minuten)<br/>\n>\n> Gleiche Aufgabe auf zwei Arten l√∂sen:<br/>\n>\n> Variante A: Eine sehr ‚Äûrohen‚Äú Prompt eingeben<br/>\n> ‚ÄûErkl√§re den Unterschied zwischen Frontend und Backend.‚Äú<br/>\n\n> Variante B ‚Äì dialogisch<br/>\n> \"Erkl√§re mir den Unterschied zwischen Frontend und Backend f√ºr einen Lernenden im 1. Lehrjahr. Stelle mir zuerst eine R√ºckfrage.\"\n>\n> Anschliessend Reflexion<br/>\n> Woher kommt:\n>\n>   - H√∂flichkeit?\n>   - Struktur?\n>   - Kontextverst√§ndnis?\n>   - Benutzerorientierung?\n>\n\n---\n\n> ## Aufgabe Ein guter Prompt\n> \n> Fragen Sie eine aktuelle Chat-KI, wie man einen guten Prompt schreibt.<br/>\n> Stellen Sie weitere Fragen, gehen Sie mit der Ki tiefer in das Thema hinein.<br/>\n> Wenden Sie dabei direkt an, was Sie gerade von der KI erkl√§rt bekommen haben.\n>\n> Tauschen Sie sich mit Ihrem Lernpartner aus.<br/>\n> Wie ein guter Prompt aufgebaut sein muss.\n> \n> Reflextion:\n>\n>   - Wieso wirkt sich ein guter Prompt so stark auf die Antwort der KI aus?\n>   - Wie kann ich das Wissen √ºber gute Prompts in Zukunft nutzen?\n>\n\n---"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"kiexplained.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","theme":["cosmo","brand"],"title":"20 KI Explained","date":"2025-12-17","date-format":"DD.MM.YYYY","author":"Peter Rutschmann"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","pdf-engine-opts":["-shell-escape"],"toc":false,"include-in-header":["../../../preamble.tex"],"output-file":"kiexplained.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"mainfont":"Arial","fontsize":"11pt","documentclass":"scrartcl","classoption":"a4paper","listings-package":{"columns":"fullflexible","break-at-whitespace":true},"title":"20 KI Explained","date":"2025-12-17","date-format":"DD.MM.YYYY","author":"Peter Rutschmann"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}